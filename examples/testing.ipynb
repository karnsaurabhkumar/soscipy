{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "def remove_none(L):\n",
    "    return [x for x in L if x is not None]\n",
    "\n",
    "class delhi_hc_search:\n",
    "    def __init__(self, case_type, case_year, case_no=None, headless=True, no_image=True, delay=1, ret=False):\n",
    "        \"\"\"\n",
    "        The Delhi High court website provides three ways to look for case data:\n",
    "        1. Using Case type\n",
    "        2. Using petitioner and respondents information\n",
    "        3. Using Advocate information\n",
    "        4. Using diary no. information\n",
    "\n",
    "        This object provides a way to look for the bulk data using case type. Hence, to initialise we require\n",
    "        three parameters\n",
    "\n",
    "        case_type: string, an element in the list of options provided by the high court website that categorises a case\n",
    "        case_no: int, case no. registered in the high court\n",
    "        case_year: int, Year of registering the case in yyyy\n",
    "        \"\"\"\n",
    "\n",
    "        self.url = \"http://delhihighcourt.nic.in/case.asp\"\n",
    "        self.case_type = case_type\n",
    "        self.case_no = case_no\n",
    "        self.case_year = str(case_year)\n",
    "\n",
    "        \"\"\"\n",
    "        Browser configuration\n",
    "        Certain optimisations have been done to reduce the chrome overload which anyways is much higher. \n",
    "        Parameters i.e. headless, no_image\n",
    "        \"\"\"\n",
    "        self.headless = headless\n",
    "        self.no_image = no_image\n",
    "        self.elements = {}\n",
    "\n",
    "        \"\"\"\n",
    "        This is the meat of the scraper which tells the code where a specific piece of content can be found\n",
    "        on the webpage.\n",
    "        \"\"\"\n",
    "        self.elements['case_type'] = \"//*[@id='InnerPageContent']/form[1]/select[1]\"\n",
    "        self.elements['case_type_elem'] = \"//*[@id='InnerPageContent']/form[1]/select[1]\"\n",
    "        self.elements['case_no'] = \"//*[@id='InnerPageContent']/form[1]/input[1]\"\n",
    "        self.elements['case_year_elem'] = \"//*[@id='c_year']\"\n",
    "        self.elements['captcha_text_input'] = \"//*[@id='inputdigit']\"\n",
    "        self.elements['captcha_text_value'] = \"//*[@id='InnerPageContent']/form[1]/label[4]\"\n",
    "        self.elements['search_but'] = \"//*[@id='InnerPageContent']/form[1]/button\"\n",
    "\n",
    "        # Keeping a global name for case_type_select element\n",
    "        self.case_type_select = None\n",
    "        self.case_type_options = None\n",
    "        self.delay = delay\n",
    "        self.ret = ret\n",
    "        self.order_subpage_data = []\n",
    "\n",
    "        # Assertions\n",
    "        if self.case_no:\n",
    "            assert type(self.case_no) == int, \"Case no. must be an integer\"\n",
    "        assert (type(case_year) == int) & (len(str(case_year)) == 4), \"Case no. must be an year in YYYY format\"\n",
    "\n",
    "    def _delay(self):\n",
    "        # Spleep function that delays page load. Usually to avoid overloading the server\n",
    "        time.sleep(self.delay)\n",
    "\n",
    "    def get_case_type_options(self):\n",
    "        # Fetches the case type options from the dropdown menue\n",
    "        case_type_select = Select(self.driver.find_element_by_xpath(self.elements['case_type']))\n",
    "        self.case_type_options = [(ind, opt.text) for ind, opt in enumerate(case_type_select.options)]\n",
    "        # print(self.case_type_options)\n",
    "\n",
    "    def get_search_page(self):\n",
    "        # Calls the setup method to create a browser driver\n",
    "        self.setup()\n",
    "        # Fetch the URL for the delhi highcourt\n",
    "        self.driver.get(self.url)\n",
    "        # Fetch all the case type options from the\n",
    "        self.get_case_type_options()\n",
    "\n",
    "    def get_captcha_text(self):\n",
    "        self.captcha_text_val = self.driver.find_element_by_xpath(self.elements['captcha_text_value'])\n",
    "        self.captcha_text_val = self.captcha_text_val.text.split(\" \")[2]\n",
    "\n",
    "    def set_case_type(self):\n",
    "        case_type_select = Select(self.driver.find_element_by_xpath(self.elements['case_type']))\n",
    "        case_type_select.select_by_index(self.case_type)\n",
    "        print(f'Selecting case type {self.case_type_options[self.case_type][1]}')\n",
    "\n",
    "    def set_case_no(self):\n",
    "        if self.case_no:\n",
    "            case_type_select = self.driver.find_element_by_xpath(self.elements['case_no'])\n",
    "            case_type_select.send_keys(self.case_no)\n",
    "            print(f'Selecting case no {self.case_no}')\n",
    "\n",
    "    def set_case_year(self):\n",
    "        case_year_select = Select(self.driver.find_element_by_xpath(self.elements['case_year_elem']))\n",
    "        case_year_select.select_by_value(self.case_year)\n",
    "\n",
    "    def set_captcha_text(self):\n",
    "        captcha_text = self.driver.find_element_by_xpath(self.elements['captcha_text_input'])\n",
    "        captcha_text.send_keys(self.captcha_text_val)\n",
    "\n",
    "    def set_param(self):\n",
    "        self.get_search_page()\n",
    "        self._delay()\n",
    "        self.set_case_type()\n",
    "        self._delay()\n",
    "        self.set_case_no()\n",
    "        self._delay()\n",
    "        self.set_case_year()\n",
    "        self._delay()\n",
    "        self.get_captcha_text()\n",
    "        self.set_captcha_text()\n",
    "\n",
    "    def get_search_results(self):\n",
    "        # Set search parameters\n",
    "        self.set_param()\n",
    "\n",
    "        # find search button and click on it\n",
    "        search_but = self.driver.find_element_by_xpath(self.elements['search_but'])\n",
    "        search_but.click()\n",
    "        self.get_search_count()\n",
    "\n",
    "    def start_scraping(self, order_links=[], pdf_links=[]):\n",
    "        self.get_search_results()\n",
    "        print(f'Total search results: {self.search_count}')\n",
    "        self.page_visited = []\n",
    "        self.page_not_visited = []\n",
    "        # seed page no visited\n",
    "        nav_links_curr_page = [element.get_attribute('href')\n",
    "                               for element in\n",
    "                               self.driver.find_elements_by_class_name('archivelink')]\n",
    "\n",
    "        for link in nav_links_curr_page:\n",
    "            self.page_not_visited.append(link)\n",
    "\n",
    "        self.valid_links = remove_none(list(set(self.page_not_visited) - set(self.page_visited)))\n",
    "        page_counter = 0\n",
    "        data = []\n",
    "        while self.valid_links:\n",
    "            print(f'No. of pages_visited: {page_counter}',end='\\r')\n",
    "            self.driver.get(self.valid_links[0])\n",
    "            self.page_visited.append(self.valid_links[0])\n",
    "            current_page = self.driver.page_source\n",
    "            # Get case status data parsed\n",
    "            dat = self.parse_status_html(current_page)\n",
    "            data.append(dat)\n",
    "            soup = bs(current_page, 'html.parser')\n",
    "            oj_details_elem = soup.find_all('button', {'class': 'button pull-right'})\n",
    "            self.oj_details = [\n",
    "                str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1], '').replace('location.href=',\n",
    "                                                                                           'http://delhihighcourt.nic.in/')\n",
    "                for elem in oj_details_elem]\n",
    "\n",
    "            main_window = self.driver.current_window_handle\n",
    "\n",
    "            for i, oj in enumerate(self.oj_details):\n",
    "                order_prefix = slugify(\n",
    "                    dat['n_{}'.format(i)]['diary_no'])  # This will later be used to name all judgement files\n",
    "\n",
    "                self.driver.execute_script(\"window.open('{}')\".format(oj))\n",
    "\n",
    "                # Assigning oj_window the current window tab put the main window in the variable. Trying to assign\n",
    "                # correct value\n",
    "                oj_window = list((set(self.driver.window_handles) - set([main_window])))[0]\n",
    "\n",
    "                self.driver.switch_to.window(oj_window)\n",
    "                order_subpage = self.driver.page_source\n",
    "                soup = bs(order_subpage, 'html.parser')\n",
    "                self.order_subpage_data.append(self.parse_orders_page(order_subpage))\n",
    "                oj_subp_elem = soup.find_all('button', {'class': 'LongCaseNoBtn'})\n",
    "                oj_subp_details = [str(elem.get('onclick')) for elem in oj_subp_elem]\n",
    "\n",
    "                # Clean string and create a list\n",
    "                for string in oj_subp_details:\n",
    "                    string = string.replace('location.href=', '')\n",
    "                    if string.startswith('\\'') and string.endswith('\\''):\n",
    "                        string = string[1:-1]\n",
    "                    order_links.append(string)\n",
    "\n",
    "                # get link to pdf_pages\n",
    "                for order in order_links:\n",
    "                    pdf_links.append(order)\n",
    "                \n",
    "                self.download_pdfs(pdf_links,order_prefix,oj_window,main_window)\n",
    "\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(main_window)\n",
    "\n",
    "            nav_links_curr_page = [element.get_attribute('href')\n",
    "                                   for element in\n",
    "                                   self.driver.find_elements_by_class_name('archivelink')]\n",
    "            for link in nav_links_curr_page:\n",
    "                if link not in self.page_visited:\n",
    "                    self.page_not_visited.append(link)\n",
    "\n",
    "            self.valid_links = remove_none(list(set(self.page_not_visited) - set(self.page_visited)))\n",
    "            page_counter += 1\n",
    "\n",
    "        self.scraped_data = pd.concat([pd.DataFrame(d).T for d in data])\n",
    "        if not self.ret:\n",
    "            return None\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def download_pdfs(self,pdf_links,order_prefix,oj_window,main_window):\n",
    "        order_count = 0\n",
    "        for link in pdf_links:\n",
    "            self.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "            pdf_win_handle = list(set(self.driver.window_handles)-set([oj_window]) - set([main_window]))[0]\n",
    "            self.driver.switch_to.window(pdf_win_handle)\n",
    "\n",
    "            url = self.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "            urllib.request.urlretrieve(url, f\"{order_prefix}_{order_count}.pdf\")\n",
    "            order_count += 1\n",
    "            self.driver.close()\n",
    "            self.driver.switch_to.window(oj_window)\n",
    "        \n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.replace(u'\\xa0', u'').replace('\\n', '')\n",
    "        text = re.sub(' +', ' ', re.sub('\\t+', '\\t', text))\n",
    "        text = text.lstrip().rstrip()\n",
    "        return text\n",
    "\n",
    "    def merge_alternate_list(self, lst1, lst2):\n",
    "        return [sub[item] for item in range(len(lst2))\n",
    "                for sub in [lst1, lst2]]\n",
    "\n",
    "    def parse_status_html(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        li_odd = soup.find_all('li', {\"class\": \"clearfix odd\"})\n",
    "        li_even = soup.find_all('li', {\"class\": \"clearfix even\"})\n",
    "        li = self.merge_alternate_list(li_odd, li_even)\n",
    "        case_status_data = {}\n",
    "        for i in range(len(li)):\n",
    "            s0 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[0].get_text())\n",
    "            sr_no = s0.replace(\".\", \"\")\n",
    "\n",
    "            s1 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[1].get_text())\n",
    "            case_status1 = re.findall(r'\\[.*?\\]', s1)[0]\n",
    "            diary_no = s1.split(case_status1)[0].replace('\\t', '')\n",
    "\n",
    "            s2 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[2].get_text())\n",
    "            advocate = s2.split('Advocate :')[-1]\n",
    "            petitioner, respondent = s2.split('Advocate :')[0].split('Vs.')\n",
    "\n",
    "            s3 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[3].get_text())\n",
    "            try:\n",
    "                court_no, case_status2, judgement_date = re.findall('^\\D*(\\d)\\s(\\D*)\\s(\\d+/\\d+/\\d+)', s3)[0]\n",
    "            except:\n",
    "                court_no, case_status2, judgement_date = \"\", \"\", \"\"\n",
    "\n",
    "            case_status_data[f'n_{i}'] = {}\n",
    "            case_status_data[f'n_{i}']['sr_no'] = sr_no\n",
    "            case_status_data[f'n_{i}']['diary_no'] = diary_no\n",
    "            case_status_data[f'n_{i}']['case_status1'] = case_status1\n",
    "            case_status_data[f'n_{i}']['petitioner'] = petitioner\n",
    "            case_status_data[f'n_{i}']['respondent'] = respondent\n",
    "            case_status_data[f'n_{i}']['advocate'] = advocate\n",
    "            case_status_data[f'n_{i}']['court_no'] = court_no\n",
    "            case_status_data[f'n_{i}']['case_status2'] = case_status2\n",
    "            case_status_data[f'n_{i}']['judgement_date'] = judgement_date\n",
    "        return case_status_data\n",
    "\n",
    "    def get_order_page_links(self, html):\n",
    "        oj_details_elem = soup.find_all('button', {'class': 'button pull-right'})\n",
    "        oj_details = [str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1], '').replace('location.href=',\n",
    "                                                                                                 'http://delhihighcourt.nic.in/')\n",
    "                      for elem in oj_details_elem]\n",
    "\n",
    "    def get_search_count(self):\n",
    "        self.search_count = self.driver.find_element_by_xpath(\"//*[@id='InnerPageContent']/span\").text\n",
    "        self.search_count = int(self.search_count.split(\":\")[-1].lstrip().rstrip())\n",
    "\n",
    "    def parse_orders_page(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        li = soup.find_all('li', {\"class\": \"clearfix odd\"})\n",
    "\n",
    "        orders = {}\n",
    "        for i in range(len(li)):\n",
    "            \"\"\"\n",
    "            The Delhi High court website essentially has a a case status page and a button\n",
    "            which gives details of all the orders related to that case. This function\n",
    "            parses the page to provide structured data from the page. Variables include\n",
    "            serial no, date of order, corrigendum text\n",
    "\n",
    "            \"\"\"\n",
    "            s0 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[0].get_text())\n",
    "            sr_no = s0.replace(\".\", \"\")\n",
    "\n",
    "            s1 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[1].get_text())\n",
    "            case_no = s1\n",
    "\n",
    "            s2 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[2].get_text())\n",
    "            date_of_order = s2\n",
    "\n",
    "            s3 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[3].get_text())\n",
    "            corrigendum = s3\n",
    "\n",
    "            orders[f'n_{i + 1}'] = {}\n",
    "            orders[f'n_{i + 1}']['sr_no'] = sr_no\n",
    "            orders[f'n_{i + 1}']['case_no'] = case_no\n",
    "            orders[f'n_{i + 1}']['date_of_order'] = date_of_order\n",
    "            orders[f'n_{i + 1}']['corrigendum'] = corrigendum\n",
    "        return orders\n",
    "\n",
    "    def setup(self):\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "\n",
    "        options.add_argument(\"--disable-xss-auditor\")\n",
    "        options.add_argument(\"--disable-web-security\")\n",
    "        options.add_argument(\"--allow-running-insecure-content\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        options.add_argument(\"--disable-webgl\")\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        profile = {\"plugins.plugins_list\": [{\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}],\n",
    "                   # Disable Chrome's PDF Viewer\n",
    "                   \"download.extensions_to_open\": \"applications/pdf\"}\n",
    "        options.add_experimental_option(\"prefs\", profile)\n",
    "\n",
    "        if self.no_image:\n",
    "            prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "            options.add_experimental_option(\"prefs\", prefs)\n",
    "            self.driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "        elif not options:\n",
    "            self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        print('Browser setup complete')\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
