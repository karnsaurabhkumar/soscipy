{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "os.environ['WDM_LOG_LEVEL'] = '0'\n",
    "\n",
    "\n",
    "class browser:\n",
    "    def __init__(self, URL=None, headless=True, no_image=True) -> object:\n",
    "        self.url = URL\n",
    "        self.headless = headless\n",
    "        self.no_image = no_image\n",
    "        self.select_elements = {}\n",
    "\n",
    "    def setup(self):\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        if self.no_image:\n",
    "            prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "            options.add_experimental_option(\"prefs\", prefs)\n",
    "            self.driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "        elif not options:\n",
    "            self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        print('starting the browser')\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "def remove_none(L):\n",
    "    return [x for x in L if x is not None]\n",
    "\n",
    "class delhi_hc_search:\n",
    "    def __init__(self, case_type, case_year, case_no=None, headless=True, no_image=True, delay=1, ret=False):\n",
    "        \"\"\"\n",
    "        The Delhi High court website provides three ways to look for case data:\n",
    "        1. Using Case type\n",
    "        2. Using petitioner and respondents information\n",
    "        3. Using Advocate information\n",
    "        4. Using diary no. information\n",
    "\n",
    "        This object provides a way to look for the bulk data using case type. Hence, to initialise we require\n",
    "        three parameters\n",
    "\n",
    "        case_type: string, an element in the list of options provided by the high court website that categorises a case\n",
    "        case_no: int, case no. registered in the high court\n",
    "        case_year: int, Year of registering the case in yyyy\n",
    "        \"\"\"\n",
    "\n",
    "        self.url = \"http://delhihighcourt.nic.in/case.asp\"\n",
    "        self.case_type = case_type\n",
    "        self.case_no = case_no\n",
    "        self.case_year = str(case_year)\n",
    "\n",
    "        \"\"\"\n",
    "        Browser configuration\n",
    "        Certain optimisations have been done to reduce the chrome overload which anyways is much higher. \n",
    "        Parameters i.e. headless, no_image\n",
    "        \"\"\"\n",
    "        self.headless = headless\n",
    "        self.no_image = no_image\n",
    "        self.elements = {}\n",
    "\n",
    "        \"\"\"\n",
    "        This is the meat of the scraper which tells the code where a specific piece of content can be found\n",
    "        on the webpage.\n",
    "        \"\"\"\n",
    "        self.elements['case_type'] = \"//*[@id='InnerPageContent']/form[1]/select[1]\"\n",
    "        self.elements['case_type_elem'] = \"//*[@id='InnerPageContent']/form[1]/select[1]\"\n",
    "        self.elements['case_no'] = \"//*[@id='InnerPageContent']/form[1]/input[1]\"\n",
    "        self.elements['case_year_elem'] = \"//*[@id='c_year']\"\n",
    "        self.elements['captcha_text_input'] = \"//*[@id='inputdigit']\"\n",
    "        self.elements['captcha_text_value'] = \"//*[@id='InnerPageContent']/form[1]/label[4]\"\n",
    "        self.elements['search_but'] = \"//*[@id='InnerPageContent']/form[1]/button\"\n",
    "\n",
    "        # Keeping a global name for case_type_select element\n",
    "        self.case_type_select = None\n",
    "        self.case_type_options = None\n",
    "        self.delay = delay\n",
    "        self.ret = ret\n",
    "        self.order_subpage_data = []\n",
    "\n",
    "        # Assertions\n",
    "        if self.case_no:\n",
    "            assert type(self.case_no) == int, \"Case no. must be an integer\"\n",
    "        assert (type(case_year) == int) & (len(str(case_year)) == 4), \"Case no. must be an year in YYYY format\"\n",
    "\n",
    "    def _delay(self):\n",
    "        # Spleep function that delays page load. Usually to avoid overloading the server\n",
    "        time.sleep(self.delay)\n",
    "\n",
    "    def get_case_type_options(self):\n",
    "        # Fetches the case type options from the dropdown menue\n",
    "        case_type_select = Select(self.driver.find_element_by_xpath(self.elements['case_type']))\n",
    "        self.case_type_options = [(ind, opt.text) for ind, opt in enumerate(case_type_select.options)]\n",
    "        # print(self.case_type_options)\n",
    "\n",
    "    def get_search_page(self):\n",
    "        # Calls the setup method to create a browser driver\n",
    "        self.setup()\n",
    "        # Fetch the URL for the delhi highcourt\n",
    "        self.driver.get(self.url)\n",
    "        # Fetch all the case type options from the\n",
    "        self.get_case_type_options()\n",
    "\n",
    "    def get_captcha_text(self):\n",
    "        self.captcha_text_val = self.driver.find_element_by_xpath(self.elements['captcha_text_value'])\n",
    "        self.captcha_text_val = self.captcha_text_val.text.split(\" \")[2]\n",
    "\n",
    "    def set_case_type(self):\n",
    "        case_type_select = Select(self.driver.find_element_by_xpath(self.elements['case_type']))\n",
    "        case_type_select.select_by_index(self.case_type)\n",
    "        print(f'Selecting case type {self.case_type_options[self.case_type][1]}')\n",
    "\n",
    "    def set_case_no(self):\n",
    "        if self.case_no:\n",
    "            case_type_select = self.driver.find_element_by_xpath(self.elements['case_no'])\n",
    "            case_type_select.send_keys(self.case_no)\n",
    "            print(f'Selecting case no {self.case_no}')\n",
    "\n",
    "    def set_case_year(self):\n",
    "        case_year_select = Select(self.driver.find_element_by_xpath(self.elements['case_year_elem']))\n",
    "        case_year_select.select_by_value(self.case_year)\n",
    "\n",
    "    def set_captcha_text(self):\n",
    "        captcha_text = self.driver.find_element_by_xpath(self.elements['captcha_text_input'])\n",
    "        captcha_text.send_keys(self.captcha_text_val)\n",
    "\n",
    "    def set_param(self):\n",
    "        self.get_search_page()\n",
    "        self._delay()\n",
    "        self.set_case_type()\n",
    "        self._delay()\n",
    "        self.set_case_no()\n",
    "        self._delay()\n",
    "        self.set_case_year()\n",
    "        self._delay()\n",
    "        self.get_captcha_text()\n",
    "        self.set_captcha_text()\n",
    "\n",
    "    def get_search_results(self):\n",
    "        # Set search parameters\n",
    "        self.set_param()\n",
    "\n",
    "        # find search button and click on it\n",
    "        search_but = self.driver.find_element_by_xpath(self.elements['search_but'])\n",
    "        search_but.click()\n",
    "        self.get_search_count()\n",
    "\n",
    "    def start_scraping(self, order_links=[], pdf_links=[]):\n",
    "        self.get_search_results()\n",
    "        print(f'Total search results: {self.search_count}')\n",
    "        self.page_visited = []\n",
    "        self.page_not_visited = []\n",
    "        # seed page no visited\n",
    "        nav_links_curr_page = [element.get_attribute('href')\n",
    "                               for element in\n",
    "                               self.driver.find_elements_by_class_name('archivelink')]\n",
    "\n",
    "        for link in nav_links_curr_page:\n",
    "            self.page_not_visited.append(link)\n",
    "\n",
    "        self.valid_links = remove_none(list(set(self.page_not_visited) - set(self.page_visited)))\n",
    "        page_counter = 0\n",
    "        data = []\n",
    "        while self.valid_links:\n",
    "            print(f'No. of pages_visited: {page_counter}',end='\\r')\n",
    "            self.driver.get(self.valid_links[0])\n",
    "            self.page_visited.append(self.valid_links[0])\n",
    "            current_page = self.driver.page_source\n",
    "            # Get case status data parsed\n",
    "            dat = self.parse_status_html(current_page)\n",
    "            data.append(dat)\n",
    "            soup = bs(current_page, 'html.parser')\n",
    "            oj_details_elem = soup.find_all('button', {'class': 'button pull-right'})\n",
    "            self.oj_details = [\n",
    "                str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1], '').replace('location.href=',\n",
    "                                                                                           'http://delhihighcourt.nic.in/')\n",
    "                for elem in oj_details_elem]\n",
    "\n",
    "            main_window = self.driver.current_window_handle\n",
    "\n",
    "            for i, oj in enumerate(self.oj_details):\n",
    "                order_prefix = slugify(\n",
    "                    dat['n_{}'.format(i)]['diary_no'])  # This will later be used to name all judgement files\n",
    "\n",
    "                self.driver.execute_script(\"window.open('{}')\".format(oj))\n",
    "\n",
    "                # Assigning oj_window the current window tab put the main window in the variable. Trying to assign\n",
    "                # correct value\n",
    "                oj_window = list((set(self.driver.window_handles) - set([main_window])))[0]\n",
    "\n",
    "                self.driver.switch_to.window(oj_window)\n",
    "                order_subpage = self.driver.page_source\n",
    "                soup = bs(order_subpage, 'html.parser')\n",
    "                self.order_subpage_data.append(self.parse_orders_page(order_subpage))\n",
    "                oj_subp_elem = soup.find_all('button', {'class': 'LongCaseNoBtn'})\n",
    "                oj_subp_details = [str(elem.get('onclick')) for elem in oj_subp_elem]\n",
    "\n",
    "                # Clean string and create a list\n",
    "                for string in oj_subp_details:\n",
    "                    string = string.replace('location.href=', '')\n",
    "                    if string.startswith('\\'') and string.endswith('\\''):\n",
    "                        string = string[1:-1]\n",
    "                    order_links.append(string)\n",
    "\n",
    "                # get link to pdf_pages\n",
    "                for order in order_links:\n",
    "                    pdf_links.append(order)\n",
    "                \n",
    "                self.download_pdfs(pdf_links,order_prefix,oj_window,main_window)\n",
    "\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(main_window)\n",
    "\n",
    "            nav_links_curr_page = [element.get_attribute('href')\n",
    "                                   for element in\n",
    "                                   self.driver.find_elements_by_class_name('archivelink')]\n",
    "            for link in nav_links_curr_page:\n",
    "                if link not in self.page_visited:\n",
    "                    self.page_not_visited.append(link)\n",
    "\n",
    "            self.valid_links = remove_none(list(set(self.page_not_visited) - set(self.page_visited)))\n",
    "            page_counter += 1\n",
    "\n",
    "        self.scraped_data = pd.concat([pd.DataFrame(d).T for d in data])\n",
    "        if not self.ret:\n",
    "            return None\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def download_pdfs(self,pdf_links,order_prefix,oj_window,main_window):\n",
    "        order_count = 0\n",
    "        for link in pdf_links:\n",
    "            self.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "            pdf_win_handle = list(set(self.driver.window_handles)-set([oj_window]) - set([main_window]))[0]\n",
    "            self.driver.switch_to.window(pdf_win_handle)\n",
    "\n",
    "            url = self.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "            urllib.request.urlretrieve(url, f\"{order_prefix}_{order_count}.pdf\")\n",
    "            order_count += 1\n",
    "            self.driver.close()\n",
    "            self.driver.switch_to.window(oj_window)\n",
    "        \n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.replace(u'\\xa0', u'').replace('\\n', '')\n",
    "        text = re.sub(' +', ' ', re.sub('\\t+', '\\t', text))\n",
    "        text = text.lstrip().rstrip()\n",
    "        return text\n",
    "\n",
    "    def merge_alternate_list(self, lst1, lst2):\n",
    "        return [sub[item] for item in range(len(lst2))\n",
    "                for sub in [lst1, lst2]]\n",
    "\n",
    "    def parse_status_html(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        li_odd = soup.find_all('li', {\"class\": \"clearfix odd\"})\n",
    "        li_even = soup.find_all('li', {\"class\": \"clearfix even\"})\n",
    "        li = self.merge_alternate_list(li_odd, li_even)\n",
    "        case_status_data = {}\n",
    "        for i in range(len(li)):\n",
    "            s0 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[0].get_text())\n",
    "            sr_no = s0.replace(\".\", \"\")\n",
    "\n",
    "            s1 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[1].get_text())\n",
    "            case_status1 = re.findall(r'\\[.*?\\]', s1)[0]\n",
    "            diary_no = s1.split(case_status1)[0].replace('\\t', '')\n",
    "\n",
    "            s2 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[2].get_text())\n",
    "            advocate = s2.split('Advocate :')[-1]\n",
    "            petitioner, respondent = s2.split('Advocate :')[0].split('Vs.')\n",
    "\n",
    "            s3 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[3].get_text())\n",
    "            try:\n",
    "                court_no, case_status2, judgement_date = re.findall('^\\D*(\\d)\\s(\\D*)\\s(\\d+/\\d+/\\d+)', s3)[0]\n",
    "            except:\n",
    "                court_no, case_status2, judgement_date = \"\", \"\", \"\"\n",
    "\n",
    "            case_status_data[f'n_{i}'] = {}\n",
    "            case_status_data[f'n_{i}']['sr_no'] = sr_no\n",
    "            case_status_data[f'n_{i}']['diary_no'] = diary_no\n",
    "            case_status_data[f'n_{i}']['case_status1'] = case_status1\n",
    "            case_status_data[f'n_{i}']['petitioner'] = petitioner\n",
    "            case_status_data[f'n_{i}']['respondent'] = respondent\n",
    "            case_status_data[f'n_{i}']['advocate'] = advocate\n",
    "            case_status_data[f'n_{i}']['court_no'] = court_no\n",
    "            case_status_data[f'n_{i}']['case_status2'] = case_status2\n",
    "            case_status_data[f'n_{i}']['judgement_date'] = judgement_date\n",
    "        return case_status_data\n",
    "\n",
    "    def get_order_page_links(self, html):\n",
    "        oj_details_elem = soup.find_all('button', {'class': 'button pull-right'})\n",
    "        oj_details = [str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1], '').replace('location.href=',\n",
    "                                                                                                 'http://delhihighcourt.nic.in/')\n",
    "                      for elem in oj_details_elem]\n",
    "\n",
    "    def get_search_count(self):\n",
    "        self.search_count = self.driver.find_element_by_xpath(\"//*[@id='InnerPageContent']/span\").text\n",
    "        self.search_count = int(self.search_count.split(\":\")[-1].lstrip().rstrip())\n",
    "\n",
    "    def parse_orders_page(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        li = soup.find_all('li', {\"class\": \"clearfix odd\"})\n",
    "\n",
    "        orders = {}\n",
    "        for i in range(len(li)):\n",
    "            \"\"\"\n",
    "            The Delhi High court website essentially has a a case status page and a button\n",
    "            which gives details of all the orders related to that case. This function\n",
    "            parses the page to provide structured data from the page. Variables include\n",
    "            serial no, date of order, corrigendum text\n",
    "\n",
    "            \"\"\"\n",
    "            s0 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[0].get_text())\n",
    "            sr_no = s0.replace(\".\", \"\")\n",
    "\n",
    "            s1 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[1].get_text())\n",
    "            case_no = s1\n",
    "\n",
    "            s2 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[2].get_text())\n",
    "            date_of_order = s2\n",
    "\n",
    "            s3 = self.clean_text(li[i].select('span', attrs={'class': re.compile('^title*')})[3].get_text())\n",
    "            corrigendum = s3\n",
    "\n",
    "            orders[f'n_{i + 1}'] = {}\n",
    "            orders[f'n_{i + 1}']['sr_no'] = sr_no\n",
    "            orders[f'n_{i + 1}']['case_no'] = case_no\n",
    "            orders[f'n_{i + 1}']['date_of_order'] = date_of_order\n",
    "            orders[f'n_{i + 1}']['corrigendum'] = corrigendum\n",
    "        return orders\n",
    "\n",
    "    def setup(self):\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "\n",
    "        options.add_argument(\"--disable-xss-auditor\")\n",
    "        options.add_argument(\"--disable-web-security\")\n",
    "        options.add_argument(\"--allow-running-insecure-content\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        options.add_argument(\"--disable-webgl\")\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        profile = {\"plugins.plugins_list\": [{\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}],\n",
    "                   # Disable Chrome's PDF Viewer\n",
    "                   \"download.extensions_to_open\": \"applications/pdf\"}\n",
    "        options.add_experimental_option(\"prefs\", profile)\n",
    "\n",
    "        if self.no_image:\n",
    "            prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "            options.add_experimental_option(\"prefs\", prefs)\n",
    "            self.driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "        elif not options:\n",
    "            self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        print('Browser setup complete')\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soscipy.utilities import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Percent Completion: [#---------] 10.00% "
     ]
    }
   ],
   "source": [
    "progress_bar.update_progress(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function -> Start scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/Users/saurabhkarn/.wdm/drivers/chromedriver/mac64/90.0.4430.24/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser setup complete\n",
      "Selecting case type BAIL APPLN. - [BAILA]\n",
      "Total search results: 4244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-522-999824ab1c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdelhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelhi_hc_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdelhi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_scraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-521-1c810f05bed9>\u001b[0m in \u001b[0;36mstart_scraping\u001b[0;34m(self, order_links, pdf_links)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mpdf_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_pdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moj_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmain_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-521-1c810f05bed9>\u001b[0m in \u001b[0;36mdownload_pdfs\u001b[0;34m(self, pdf_links, order_prefix, oj_window, main_window)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/html/body/iframe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{order_prefix}_{order_count}.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0morder_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delhi = delhi_hc_search(case_type=4,case_no=None,case_year=2020,headless=False,delay=0)\n",
    "delhi.start_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1}"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a - b - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close the driver\n",
    "delhi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/Users/saurabhkarn/.wdm/drivers/chromedriver/mac64/90.0.4430.24/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser setup complete\n",
      "Selecting case type BAIL APPLN. - [BAILA]\n",
      "Total search results: 4244\n"
     ]
    }
   ],
   "source": [
    "#Step1: Reach the first page\n",
    "delhi.get_search_results()\n",
    "print(f'Total search results: {delhi.search_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while self.valid_links:\n",
    "    self.driver.get(self.valid_links[0])\n",
    "    self.page_visited.append(self.valid_links[0])\n",
    "    current_page = self.driver.page_source\n",
    "    #Get case status data parsed\n",
    "    dat = self.parse_status_html(current_page)\n",
    "    data.append(dat)\n",
    "    order_prefix = slugify(dat['n_{}'.format(0)]['diary_no']) #This will later be used to name all judgement files\n",
    "    order_count = 0\n",
    "    soup = bs(current_page,'html5lib')\n",
    "    oj_details_elem = soup.find_all('button',{'class':'button pull-right'})\n",
    "    oj_details = [str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1],'').replace('location.href=','http://delhihighcourt.nic.in/')\n",
    "                    for elem in oj_details_elem]\n",
    "\n",
    "    main_window = delhi.driver.current_window_handle\n",
    "    \n",
    "    for oj_det in oj_details:\n",
    "        delhi.driver.execute_script(\"window.open('{}')\".format(oj_det))\n",
    "        delhi.driver.switch_to.window(delhi.driver.window_handles[1])\n",
    "        order_subpage = delhi.driver.page_source\n",
    "        soup = bs(order_subpage,'html5lib')\n",
    "        oj_subp_elem = soup.find_all('button',{'class':'LongCaseNoBtn'})\n",
    "        oj_subp_details = [str(elem.get('onclick')) for elem in oj_subp_elem]\n",
    "        order_links=[]\n",
    "\n",
    "        #Clean string and create a list\n",
    "        for string in oj_subp_details:\n",
    "            string = string.replace('location.href=','')\n",
    "            if string.startswith('\\'') and string.endswith('\\''):\n",
    "                string = string[1:-1]\n",
    "            order_links.append(string)\n",
    "\n",
    "        #get link to pdf_pages\n",
    "        for order in order_links:\n",
    "            pdf_links.append(order)\n",
    "\n",
    "        for link in pdf_links:\n",
    "            delhi.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "            window_handles = delhi.driver.window_handles\n",
    "            delhi.driver.switch_to.window(window_handles[len(window_handles)-1])\n",
    "            url = delhi.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "            urllib.request.urlretrieve(url, f\"{order_prefix}_{order_count}.pdf\")\n",
    "            delhi.driver.close()\n",
    "            delhi.driver.switch_to.window(delhi.driver.window_handles[0])\n",
    "            \n",
    "        delhi.driver.close()\n",
    "        delhi.driver.switch_to.window(main_window)\n",
    "            \n",
    "\n",
    "            \n",
    "            #Get order's link\n",
    "            \n",
    "            #get information of orders\n",
    "            \n",
    "            #get links to PDFs\n",
    "            \n",
    "            #save PDFs in a folder\n",
    "            \n",
    "            #Get other navigation links\n",
    "            nav_links_curr_page = [element.get_attribute('href') \n",
    "                           for element in \n",
    "                           self.driver.find_elements_by_class_name('archivelink')]\n",
    "            for link in nav_links_curr_page:\n",
    "                if link not in self.page_visited:\n",
    "                    self.page_not_visited.append(link)\n",
    "            self.valid_links = remove_none(list(set(self.page_not_visited) - set(self.page_visited)))\n",
    "            page_counter += 1\n",
    "            if page_counter == 3:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = delhi.driver.page_source\n",
    "dat = delhi.parse_status_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_prefix = slugify(dat['n_{}'.format(0)]['diary_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the current page for judgement link\n",
    "#dat = delhi.parse_status_html(html)\n",
    "soup = bs(html,'html5lib')\n",
    "oj_details_elem = soup.find_all('button',{'class':'button pull-right'})\n",
    "oj_details = [str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1],'').replace('location.href=','http://delhihighcourt.nic.in/')\n",
    "                for elem in oj_details_elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the first judgement link\n",
    "delhi.driver.execute_script(\"window.open('{}')\".format(oj_details[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch to oder window\n",
    "delhi.driver.switch_to.window(delhi.driver.window_handles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_subpage = delhi.driver.page_source\n",
    "soup = bs(order_subpage,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_subp_elem = soup.find_all('button',{'class':'LongCaseNoBtn'})\n",
    "oj_subp_details = [str(elem.get('onclick')) for elem in oj_subp_elem]\n",
    "order_links=[]\n",
    "\n",
    "#Clean string and create a list\n",
    "for string in oj_subp_details:\n",
    "    string = string.replace('location.href=','')\n",
    "    if string.startswith('\\'') and string.endswith('\\''):\n",
    "        string = string[1:-1]\n",
    "    order_links.append(string)\n",
    "\n",
    "#get link to pdf_pages\n",
    "for order in order_links:\n",
    "    pdf_links.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.switch_to.window(delhi.driver.window_handles[1])\n",
    "html = delhi.driver.page_source\n",
    "soup = bs(html,'html5lib')\n",
    "oj_details_elem = soup.find_all('button',{'class':'LongCaseNoBtn'})\n",
    "oj_details = [str(elem.get('onclick')) for elem in oj_details_elem]\n",
    "\n",
    "order_links=[]\n",
    "for string in oj_details:\n",
    "    string = string.replace('location.href=','')\n",
    "    if string.startswith('\\'') and string.endswith('\\''):\n",
    "        string = string[1:-1]\n",
    "    order_links.append(string)\n",
    "\n",
    "for order in order_links:\n",
    "    pdf_links.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = pdf_links[0]\n",
    "delhi.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "window_handles = delhi.driver.window_handles\n",
    "delhi.driver.switch_to.window(window_handles[len(window_handles)-1])\n",
    "\n",
    "url = delhi.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "urllib.request.urlretrieve(url, f\"{order_prefix}_{c}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for link in pdf_links:\n",
    "    delhi.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "    window_handles = delhi.driver.window_handles\n",
    "    delhi.driver.switch_to.window(window_handles[len(window_handles)-1])\n",
    "    url = delhi.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "    urllib.request.urlretrieve(url, f\"filename_{c}.pdf\")\n",
    "    c += 1\n",
    "    delhi.driver.close()\n",
    "    delhi.driver.switch_to.window(delhi.driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2: Seed a list of valid navigation link\n",
    "delhi.driver.page_visited = []\n",
    "delhi.driver.page_not_visited = []\n",
    "#seed page no visited\n",
    "nav_links_curr_page = [element.get_attribute('href') \n",
    "                       for element in \n",
    "                       delhi.driver.driver.find_elements_by_class_name('archivelink')]\n",
    "\n",
    "for link in nav_links_curr_page:\n",
    "    delhi.driver.page_not_visited.append(link)\n",
    "\n",
    "delhi.driver.valid_links = remove_none(list(set(delhi.driver.page_not_visited) - set(delhi.driver.page_visited)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Loop over the valid links. For testing only the first page is being called\n",
    "self.driver.get(self.valid_links[0])\n",
    "self.page_visited.append(self.valid_links[0])\n",
    "current_page = self.driver.page_source\n",
    "#Get case status data parsed\n",
    "data.append(self.parse_status_html(current_page))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_counter = 0\n",
    "data = []\n",
    "while self.valid_links:\n",
    "    self.driver.get(self.valid_links[0])\n",
    "    self.page_visited.append(self.valid_links[0])\n",
    "    current_page = self.driver.page_source\n",
    "    #Get case status data parsed\n",
    "    data.append(self.parse_status_html(current_page))\n",
    "\n",
    "    #Get order's link\n",
    "\n",
    "    #get information of orders\n",
    "\n",
    "    #get links to PDFs\n",
    "\n",
    "    #save PDFs in a folder\n",
    "\n",
    "    #Get other navigation links\n",
    "    nav_links_curr_page = [element.get_attribute('href') \n",
    "                   for element in \n",
    "                   self.driver.find_elements_by_class_name('archivelink')]\n",
    "    for link in nav_links_curr_page:\n",
    "        if link not in self.page_visited:\n",
    "            self.page_not_visited.append(link)\n",
    "    self.valid_links = remove_none(list(set(self.page_not_visited) - set(self.page_visited)))\n",
    "    page_counter += 1\n",
    "    if page_counter == 3:\n",
    "        break\n",
    "\n",
    "\n",
    "self.scraped_data = pd.concat([pd.DataFrame(d).T for d in data])\n",
    "if not self.ret:\n",
    "    return None\n",
    "else:\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi = delhi_hc_search(case_type=4,case_no=None,case_year=2020,headless=False,delay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [/Users/saurabhkarn/.wdm/drivers/chromedriver/mac64/90.0.4430.24/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser setup complete\n",
      "Selecting case type BAIL APPLN. - [BAILA]\n",
      "Total search results: 4244\n"
     ]
    }
   ],
   "source": [
    "data = delhi.start_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat = delhi.parse_status_html(html)\n",
    "html = delhi.driver.page_source\n",
    "soup = bs(html,'html5lib')\n",
    "oj_details_elem = soup.find_all('button',{'class':'button pull-right'})\n",
    "oj_details = [str(elem.get('onclick')).replace(str(elem.get('onclick'))[-1],'').replace('location.href=','http://delhihighcourt.nic.in/')\n",
    "                for elem in oj_details_elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in oj_details:\n",
    "    delhi.driver.execute_script(\"window.open('{}')\".format(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_order_pdf():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = delhi.driver.window_handles\n",
    "c = 0\n",
    "pdf_links = []\n",
    "for win in windows[1:]:\n",
    "    delhi.driver.switch_to.window(win)\n",
    "    html = delhi.driver.page_source\n",
    "    soup = bs(html,'html5lib')\n",
    "    oj_details_elem = soup.find_all('button',{'class':'LongCaseNoBtn'})\n",
    "    oj_details = [str(elem.get('onclick')) for elem in oj_details_elem]\n",
    "\n",
    "    order_links=[]\n",
    "    for string in oj_details:\n",
    "        string = string.replace('location.href=','')\n",
    "        if string.startswith('\\'') and string.endswith('\\''):\n",
    "            string = string[1:-1]\n",
    "        order_links.append(string)\n",
    "    \n",
    "    for order in order_links:\n",
    "        pdf_links.append(order)        \n",
    "    \n",
    "    delhi.driver.switch_to.window(windows[0])\n",
    "    delhi.driver.close()\n",
    "    \n",
    "for link in pdf_links:\n",
    "    delhi.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "    window_handles = delhi.driver.window_handles\n",
    "    delhi.driver.switch_to.window(window_handles[len(window_handles)-1])\n",
    "    url = delhi.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "    urllib.request.urlretrieve(url, f\"filename_{c}.pdf\")\n",
    "    c += 1\n",
    "    delhi.driver.close()\n",
    "    delhi.driver.switch_to.window(delhi.driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CDwindow-95098CBFD64872591C5043BCCCFB54DD']"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delhi.driver.window_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.switch_to.window(delhi.driver.window_handles[0])\n",
    "delhi.driver.execute_script(\"window.open('{}')\".format(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.close()\n",
    "delhi.driver.switch_to.window(delhi.driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in pdf_links:\n",
    "    delhi.driver.execute_script(\"window.open('{}')\".format(link))\n",
    "    window_handles = delhi.driver.window_handles\n",
    "    delhi.driver.switch_to.window(window_handles[len(window_handles)-1])\n",
    "    url = delhi.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')\n",
    "    urllib.request.urlretrieve(url, f\"filename_{c}.pdf\")\n",
    "    c += 1\n",
    "    delhi.driver.close()\n",
    "    delhi.driver.switch_to.window(delhi.driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = delhi.driver.window_handles\n",
    "delhi.driver.switch_to.window(windows[1])\n",
    "delhi.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.switch_to.window(windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(windows)):\n",
    "    time.sleep(5)\n",
    "    delhi.driver.switch_to.window(windows[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.switch_to.window(windows[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = delhi.driver.page_source\n",
    "soup = bs(html,'html5lib')\n",
    "oj_details_elem = soup.find_all('button',{'class':'LongCaseNoBtn'})\n",
    "oj_details = [str(elem.get('onclick')) for elem in oj_details_elem]\n",
    "\n",
    "order_links=[]\n",
    "for string in oj_details:\n",
    "    string = string.replace('location.href=','')\n",
    "    if string.startswith('\\'') and string.endswith('\\''):\n",
    "        string = string[1:-1]\n",
    "    order_links.append(string)\n",
    "\n",
    "for order in order_links:\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi.driver.execute_script(\"window.open('{}')\".format(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = delhi.driver.window_handles\n",
    "delhi.driver.switch_to.window(windows[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = delhi.driver.find_element_by_xpath(\"/html/body/iframe\").get_attribute('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('filename.pdf', <http.client.HTTPMessage at 0x7fb294916c10>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(url, \"filename.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_elem = '/html/body/pdf-viewer//div/div[2]/div[1]/div[2]/embed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from link: http://delhihighcourt.nic.in/dhcqrydisp_o.asp?pn=82812&yr=2020\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/Users/saurabhkarn/.wdm/drivers/chromedriver/mac64/90.0.4430.24/chromedriver] found in cache\n",
      "<ipython-input-272-c0c0f959f8f5>:19: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options = options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: test_file\n",
      "Status: Download Complete.\n",
      "Folder: /Users/saurabhkarn\n"
     ]
    }
   ],
   "source": [
    "download_pdf(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = delhi.driver.page_source\n",
    "soup = bs(html,'html5lib')\n",
    "oj_details_elem = soup.find_all('plugin',{'class':'LongCaseNoBtn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_1': {'sr_no': '1',\n",
       "  'case_no': 'BAIL APPLN. 973/2020',\n",
       "  'date_of_order': '09/06/2020',\n",
       "  'corrigendum': ''},\n",
       " 'n_2': {'sr_no': '2',\n",
       "  'case_no': 'BAIL APPLN. 973/2020',\n",
       "  'date_of_order': '08/06/2020',\n",
       "  'corrigendum': ''},\n",
       " 'n_3': {'sr_no': '3',\n",
       "  'case_no': 'BAIL APPLN. 973/2020',\n",
       "  'date_of_order': '05/06/2020',\n",
       "  'corrigendum': ''},\n",
       " 'n_4': {'sr_no': '4',\n",
       "  'case_no': 'BAIL APPLN. 973/2020',\n",
       "  'date_of_order': '28/05/2020',\n",
       "  'corrigendum': ''},\n",
       " 'n_5': {'sr_no': '5',\n",
       "  'case_no': 'BAIL APPLN. 973/2020',\n",
       "  'date_of_order': '21/05/2020',\n",
       "  'corrigendum': ''}}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delhi.parse_orders_page(delhi.driver.page_source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
